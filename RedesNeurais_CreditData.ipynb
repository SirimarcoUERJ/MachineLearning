{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJ8d4PaLOA22RJ+q7vG6kB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SirimarcoUERJ/MachineLearning/blob/main/RedesNeurais_CreditData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rede Neurais Artificiais aplicadas a base de dados do credito.**"
      ],
      "metadata": {
        "id": "eW90IkPhF79N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OYAYXMhWFyhu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pickle as pkl\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUZQrw8OGZOs",
        "outputId": "497d95e5-c991-43d4-de5c-0572ec413b08"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arq = \"/content/drive/MyDrive/Colab Notebooks/Machine Learn/data/credit.pkl\"\n",
        "with open(arq, \"rb\") as f:\n",
        "  x_treino, y_treino, x_teste, y_teste = pkl.load(f)"
      ],
      "metadata": {
        "id": "41a_kfq3HFr_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_treino.shape, y_treino.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeFoNOsLH0_J",
        "outputId": "ec520afe-cf3f-4da1-98ba-cdfcdabcba2a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1500, 3), (1500,))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_teste.shape, y_teste.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QajHyArH4VD",
        "outputId": "280d4b12-141d-4c0f-ec53-1cd4498f5142"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 3), (500,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rn_credit = MLPClassifier(max_iter=1500, verbose=True, hidden_layer_sizes=(2,2))\n",
        "rn_credit.fit(x_treino, y_treino)\n",
        "\n",
        "previsoes = rn_credit.predict(x_teste)\n",
        "previsoes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNlwg_JHH7gc",
        "outputId": "9df3719b-5485-43fd-9769-9fb9a1b65e57"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.53972263\n",
            "Iteration 2, loss = 0.53277621\n",
            "Iteration 3, loss = 0.52636063\n",
            "Iteration 4, loss = 0.52041674\n",
            "Iteration 5, loss = 0.51484676\n",
            "Iteration 6, loss = 0.50963024\n",
            "Iteration 7, loss = 0.50460372\n",
            "Iteration 8, loss = 0.49987215\n",
            "Iteration 9, loss = 0.49540559\n",
            "Iteration 10, loss = 0.49096585\n",
            "Iteration 11, loss = 0.48677104\n",
            "Iteration 12, loss = 0.48278739\n",
            "Iteration 13, loss = 0.47894019\n",
            "Iteration 14, loss = 0.47518103\n",
            "Iteration 15, loss = 0.47159778\n",
            "Iteration 16, loss = 0.46806206\n",
            "Iteration 17, loss = 0.46469239\n",
            "Iteration 18, loss = 0.46131142\n",
            "Iteration 19, loss = 0.45798991\n",
            "Iteration 20, loss = 0.45472148\n",
            "Iteration 21, loss = 0.45162216\n",
            "Iteration 22, loss = 0.44854417\n",
            "Iteration 23, loss = 0.44552750\n",
            "Iteration 24, loss = 0.44249809\n",
            "Iteration 25, loss = 0.43952982\n",
            "Iteration 26, loss = 0.43657582\n",
            "Iteration 27, loss = 0.43361156\n",
            "Iteration 28, loss = 0.43067698\n",
            "Iteration 29, loss = 0.42769439\n",
            "Iteration 30, loss = 0.42470396\n",
            "Iteration 31, loss = 0.42171812\n",
            "Iteration 32, loss = 0.41861750\n",
            "Iteration 33, loss = 0.41567737\n",
            "Iteration 34, loss = 0.41261036\n",
            "Iteration 35, loss = 0.40961212\n",
            "Iteration 36, loss = 0.40661316\n",
            "Iteration 37, loss = 0.40366349\n",
            "Iteration 38, loss = 0.40075227\n",
            "Iteration 39, loss = 0.39775575\n",
            "Iteration 40, loss = 0.39479273\n",
            "Iteration 41, loss = 0.39178719\n",
            "Iteration 42, loss = 0.38870072\n",
            "Iteration 43, loss = 0.38573687\n",
            "Iteration 44, loss = 0.38275537\n",
            "Iteration 45, loss = 0.37981763\n",
            "Iteration 46, loss = 0.37698691\n",
            "Iteration 47, loss = 0.37403717\n",
            "Iteration 48, loss = 0.37116852\n",
            "Iteration 49, loss = 0.36825458\n",
            "Iteration 50, loss = 0.36518039\n",
            "Iteration 51, loss = 0.36219477\n",
            "Iteration 52, loss = 0.35909637\n",
            "Iteration 53, loss = 0.35605873\n",
            "Iteration 54, loss = 0.35301458\n",
            "Iteration 55, loss = 0.34987119\n",
            "Iteration 56, loss = 0.34689020\n",
            "Iteration 57, loss = 0.34382499\n",
            "Iteration 58, loss = 0.34081757\n",
            "Iteration 59, loss = 0.33779075\n",
            "Iteration 60, loss = 0.33476821\n",
            "Iteration 61, loss = 0.33170882\n",
            "Iteration 62, loss = 0.32861918\n",
            "Iteration 63, loss = 0.32550002\n",
            "Iteration 64, loss = 0.32245789\n",
            "Iteration 65, loss = 0.31934540\n",
            "Iteration 66, loss = 0.31626425\n",
            "Iteration 67, loss = 0.31308133\n",
            "Iteration 68, loss = 0.31002939\n",
            "Iteration 69, loss = 0.30705501\n",
            "Iteration 70, loss = 0.30412758\n",
            "Iteration 71, loss = 0.30128433\n",
            "Iteration 72, loss = 0.29865557\n",
            "Iteration 73, loss = 0.29602491\n",
            "Iteration 74, loss = 0.29353954\n",
            "Iteration 75, loss = 0.29107671\n",
            "Iteration 76, loss = 0.28878891\n",
            "Iteration 77, loss = 0.28647361\n",
            "Iteration 78, loss = 0.28432773\n",
            "Iteration 79, loss = 0.28226852\n",
            "Iteration 80, loss = 0.28020051\n",
            "Iteration 81, loss = 0.27827788\n",
            "Iteration 82, loss = 0.27633721\n",
            "Iteration 83, loss = 0.27447536\n",
            "Iteration 84, loss = 0.27268836\n",
            "Iteration 85, loss = 0.27093849\n",
            "Iteration 86, loss = 0.26923551\n",
            "Iteration 87, loss = 0.26760554\n",
            "Iteration 88, loss = 0.26603693\n",
            "Iteration 89, loss = 0.26447410\n",
            "Iteration 90, loss = 0.26300572\n",
            "Iteration 91, loss = 0.26149478\n",
            "Iteration 92, loss = 0.26008044\n",
            "Iteration 93, loss = 0.25868892\n",
            "Iteration 94, loss = 0.25735739\n",
            "Iteration 95, loss = 0.25597797\n",
            "Iteration 96, loss = 0.25464477\n",
            "Iteration 97, loss = 0.25337186\n",
            "Iteration 98, loss = 0.25209044\n",
            "Iteration 99, loss = 0.25083717\n",
            "Iteration 100, loss = 0.24963681\n",
            "Iteration 101, loss = 0.24841987\n",
            "Iteration 102, loss = 0.24721619\n",
            "Iteration 103, loss = 0.24606201\n",
            "Iteration 104, loss = 0.24490178\n",
            "Iteration 105, loss = 0.24377350\n",
            "Iteration 106, loss = 0.24263779\n",
            "Iteration 107, loss = 0.24150848\n",
            "Iteration 108, loss = 0.24039867\n",
            "Iteration 109, loss = 0.23931845\n",
            "Iteration 110, loss = 0.23826075\n",
            "Iteration 111, loss = 0.23717463\n",
            "Iteration 112, loss = 0.23610617\n",
            "Iteration 113, loss = 0.23505236\n",
            "Iteration 114, loss = 0.23399938\n",
            "Iteration 115, loss = 0.23296121\n",
            "Iteration 116, loss = 0.23193655\n",
            "Iteration 117, loss = 0.23088714\n",
            "Iteration 118, loss = 0.22984688\n",
            "Iteration 119, loss = 0.22885471\n",
            "Iteration 120, loss = 0.22782500\n",
            "Iteration 121, loss = 0.22682527\n",
            "Iteration 122, loss = 0.22584563\n",
            "Iteration 123, loss = 0.22486950\n",
            "Iteration 124, loss = 0.22386943\n",
            "Iteration 125, loss = 0.22291399\n",
            "Iteration 126, loss = 0.22194078\n",
            "Iteration 127, loss = 0.22099642\n",
            "Iteration 128, loss = 0.22005874\n",
            "Iteration 129, loss = 0.21910217\n",
            "Iteration 130, loss = 0.21819650\n",
            "Iteration 131, loss = 0.21723758\n",
            "Iteration 132, loss = 0.21639031\n",
            "Iteration 133, loss = 0.21540773\n",
            "Iteration 134, loss = 0.21451819\n",
            "Iteration 135, loss = 0.21361505\n",
            "Iteration 136, loss = 0.21273750\n",
            "Iteration 137, loss = 0.21185266\n",
            "Iteration 138, loss = 0.21093503\n",
            "Iteration 139, loss = 0.21008701\n",
            "Iteration 140, loss = 0.20920459\n",
            "Iteration 141, loss = 0.20831588\n",
            "Iteration 142, loss = 0.20744922\n",
            "Iteration 143, loss = 0.20657878\n",
            "Iteration 144, loss = 0.20571060\n",
            "Iteration 145, loss = 0.20486763\n",
            "Iteration 146, loss = 0.20401798\n",
            "Iteration 147, loss = 0.20315150\n",
            "Iteration 148, loss = 0.20232280\n",
            "Iteration 149, loss = 0.20146829\n",
            "Iteration 150, loss = 0.20065120\n",
            "Iteration 151, loss = 0.19982019\n",
            "Iteration 152, loss = 0.19901337\n",
            "Iteration 153, loss = 0.19818955\n",
            "Iteration 154, loss = 0.19738400\n",
            "Iteration 155, loss = 0.19657424\n",
            "Iteration 156, loss = 0.19576851\n",
            "Iteration 157, loss = 0.19496305\n",
            "Iteration 158, loss = 0.19417842\n",
            "Iteration 159, loss = 0.19336289\n",
            "Iteration 160, loss = 0.19257564\n",
            "Iteration 161, loss = 0.19178426\n",
            "Iteration 162, loss = 0.19101698\n",
            "Iteration 163, loss = 0.19025530\n",
            "Iteration 164, loss = 0.18949584\n",
            "Iteration 165, loss = 0.18873494\n",
            "Iteration 166, loss = 0.18798317\n",
            "Iteration 167, loss = 0.18724890\n",
            "Iteration 168, loss = 0.18646986\n",
            "Iteration 169, loss = 0.18571335\n",
            "Iteration 170, loss = 0.18494555\n",
            "Iteration 171, loss = 0.18424636\n",
            "Iteration 172, loss = 0.18349964\n",
            "Iteration 173, loss = 0.18276770\n",
            "Iteration 174, loss = 0.18202876\n",
            "Iteration 175, loss = 0.18128088\n",
            "Iteration 176, loss = 0.18055992\n",
            "Iteration 177, loss = 0.17984680\n",
            "Iteration 178, loss = 0.17910217\n",
            "Iteration 179, loss = 0.17840560\n",
            "Iteration 180, loss = 0.17769610\n",
            "Iteration 181, loss = 0.17698536\n",
            "Iteration 182, loss = 0.17622262\n",
            "Iteration 183, loss = 0.17551877\n",
            "Iteration 184, loss = 0.17481670\n",
            "Iteration 185, loss = 0.17412387\n",
            "Iteration 186, loss = 0.17341860\n",
            "Iteration 187, loss = 0.17271195\n",
            "Iteration 188, loss = 0.17200890\n",
            "Iteration 189, loss = 0.17133466\n",
            "Iteration 190, loss = 0.17065115\n",
            "Iteration 191, loss = 0.16993477\n",
            "Iteration 192, loss = 0.16926319\n",
            "Iteration 193, loss = 0.16855769\n",
            "Iteration 194, loss = 0.16787915\n",
            "Iteration 195, loss = 0.16718269\n",
            "Iteration 196, loss = 0.16650477\n",
            "Iteration 197, loss = 0.16584673\n",
            "Iteration 198, loss = 0.16517620\n",
            "Iteration 199, loss = 0.16449975\n",
            "Iteration 200, loss = 0.16383333\n",
            "Iteration 201, loss = 0.16318659\n",
            "Iteration 202, loss = 0.16254453\n",
            "Iteration 203, loss = 0.16187105\n",
            "Iteration 204, loss = 0.16118842\n",
            "Iteration 205, loss = 0.16055382\n",
            "Iteration 206, loss = 0.15992026\n",
            "Iteration 207, loss = 0.15926729\n",
            "Iteration 208, loss = 0.15861440\n",
            "Iteration 209, loss = 0.15798862\n",
            "Iteration 210, loss = 0.15735551\n",
            "Iteration 211, loss = 0.15666356\n",
            "Iteration 212, loss = 0.15609547\n",
            "Iteration 213, loss = 0.15548553\n",
            "Iteration 214, loss = 0.15481595\n",
            "Iteration 215, loss = 0.15418565\n",
            "Iteration 216, loss = 0.15356427\n",
            "Iteration 217, loss = 0.15292009\n",
            "Iteration 218, loss = 0.15231060\n",
            "Iteration 219, loss = 0.15169807\n",
            "Iteration 220, loss = 0.15105757\n",
            "Iteration 221, loss = 0.15043825\n",
            "Iteration 222, loss = 0.14982674\n",
            "Iteration 223, loss = 0.14921373\n",
            "Iteration 224, loss = 0.14862118\n",
            "Iteration 225, loss = 0.14798801\n",
            "Iteration 226, loss = 0.14738723\n",
            "Iteration 227, loss = 0.14678419\n",
            "Iteration 228, loss = 0.14619911\n",
            "Iteration 229, loss = 0.14559880\n",
            "Iteration 230, loss = 0.14499666\n",
            "Iteration 231, loss = 0.14441078\n",
            "Iteration 232, loss = 0.14381223\n",
            "Iteration 233, loss = 0.14323954\n",
            "Iteration 234, loss = 0.14271026\n",
            "Iteration 235, loss = 0.14206050\n",
            "Iteration 236, loss = 0.14146308\n",
            "Iteration 237, loss = 0.14087901\n",
            "Iteration 238, loss = 0.14030218\n",
            "Iteration 239, loss = 0.13973132\n",
            "Iteration 240, loss = 0.13917424\n",
            "Iteration 241, loss = 0.13860972\n",
            "Iteration 242, loss = 0.13810481\n",
            "Iteration 243, loss = 0.13749317\n",
            "Iteration 244, loss = 0.13692016\n",
            "Iteration 245, loss = 0.13637073\n",
            "Iteration 246, loss = 0.13576859\n",
            "Iteration 247, loss = 0.13522168\n",
            "Iteration 248, loss = 0.13472896\n",
            "Iteration 249, loss = 0.13413048\n",
            "Iteration 250, loss = 0.13370170\n",
            "Iteration 251, loss = 0.13304128\n",
            "Iteration 252, loss = 0.13248074\n",
            "Iteration 253, loss = 0.13192282\n",
            "Iteration 254, loss = 0.13144859\n",
            "Iteration 255, loss = 0.13088546\n",
            "Iteration 256, loss = 0.13048093\n",
            "Iteration 257, loss = 0.12981644\n",
            "Iteration 258, loss = 0.12930674\n",
            "Iteration 259, loss = 0.12876691\n",
            "Iteration 260, loss = 0.12825793\n",
            "Iteration 261, loss = 0.12773245\n",
            "Iteration 262, loss = 0.12721695\n",
            "Iteration 263, loss = 0.12670038\n",
            "Iteration 264, loss = 0.12621398\n",
            "Iteration 265, loss = 0.12569354\n",
            "Iteration 266, loss = 0.12516636\n",
            "Iteration 267, loss = 0.12466364\n",
            "Iteration 268, loss = 0.12414145\n",
            "Iteration 269, loss = 0.12364099\n",
            "Iteration 270, loss = 0.12313581\n",
            "Iteration 271, loss = 0.12266758\n",
            "Iteration 272, loss = 0.12221903\n",
            "Iteration 273, loss = 0.12166719\n",
            "Iteration 274, loss = 0.12115979\n",
            "Iteration 275, loss = 0.12072689\n",
            "Iteration 276, loss = 0.12019762\n",
            "Iteration 277, loss = 0.11970701\n",
            "Iteration 278, loss = 0.11923719\n",
            "Iteration 279, loss = 0.11874054\n",
            "Iteration 280, loss = 0.11828479\n",
            "Iteration 281, loss = 0.11778156\n",
            "Iteration 282, loss = 0.11737308\n",
            "Iteration 283, loss = 0.11696699\n",
            "Iteration 284, loss = 0.11641943\n",
            "Iteration 285, loss = 0.11590946\n",
            "Iteration 286, loss = 0.11547450\n",
            "Iteration 287, loss = 0.11506067\n",
            "Iteration 288, loss = 0.11451639\n",
            "Iteration 289, loss = 0.11410203\n",
            "Iteration 290, loss = 0.11364261\n",
            "Iteration 291, loss = 0.11315691\n",
            "Iteration 292, loss = 0.11272353\n",
            "Iteration 293, loss = 0.11230766\n",
            "Iteration 294, loss = 0.11182449\n",
            "Iteration 295, loss = 0.11138470\n",
            "Iteration 296, loss = 0.11093209\n",
            "Iteration 297, loss = 0.11056765\n",
            "Iteration 298, loss = 0.11013308\n",
            "Iteration 299, loss = 0.10963591\n",
            "Iteration 300, loss = 0.10920619\n",
            "Iteration 301, loss = 0.10873899\n",
            "Iteration 302, loss = 0.10833222\n",
            "Iteration 303, loss = 0.10791285\n",
            "Iteration 304, loss = 0.10750020\n",
            "Iteration 305, loss = 0.10708452\n",
            "Iteration 306, loss = 0.10664463\n",
            "Iteration 307, loss = 0.10620740\n",
            "Iteration 308, loss = 0.10578766\n",
            "Iteration 309, loss = 0.10540592\n",
            "Iteration 310, loss = 0.10498551\n",
            "Iteration 311, loss = 0.10454628\n",
            "Iteration 312, loss = 0.10414417\n",
            "Iteration 313, loss = 0.10371926\n",
            "Iteration 314, loss = 0.10330719\n",
            "Iteration 315, loss = 0.10290273\n",
            "Iteration 316, loss = 0.10251409\n",
            "Iteration 317, loss = 0.10216709\n",
            "Iteration 318, loss = 0.10172044\n",
            "Iteration 319, loss = 0.10130967\n",
            "Iteration 320, loss = 0.10091128\n",
            "Iteration 321, loss = 0.10053337\n",
            "Iteration 322, loss = 0.10014406\n",
            "Iteration 323, loss = 0.09975667\n",
            "Iteration 324, loss = 0.09937162\n",
            "Iteration 325, loss = 0.09898095\n",
            "Iteration 326, loss = 0.09859346\n",
            "Iteration 327, loss = 0.09821465\n",
            "Iteration 328, loss = 0.09783606\n",
            "Iteration 329, loss = 0.09745052\n",
            "Iteration 330, loss = 0.09712055\n",
            "Iteration 331, loss = 0.09674459\n",
            "Iteration 332, loss = 0.09635550\n",
            "Iteration 333, loss = 0.09599069\n",
            "Iteration 334, loss = 0.09560585\n",
            "Iteration 335, loss = 0.09524907\n",
            "Iteration 336, loss = 0.09487396\n",
            "Iteration 337, loss = 0.09449146\n",
            "Iteration 338, loss = 0.09419932\n",
            "Iteration 339, loss = 0.09380104\n",
            "Iteration 340, loss = 0.09342180\n",
            "Iteration 341, loss = 0.09307535\n",
            "Iteration 342, loss = 0.09270823\n",
            "Iteration 343, loss = 0.09235219\n",
            "Iteration 344, loss = 0.09203020\n",
            "Iteration 345, loss = 0.09164461\n",
            "Iteration 346, loss = 0.09128959\n",
            "Iteration 347, loss = 0.09092745\n",
            "Iteration 348, loss = 0.09060834\n",
            "Iteration 349, loss = 0.09030817\n",
            "Iteration 350, loss = 0.08996260\n",
            "Iteration 351, loss = 0.08956474\n",
            "Iteration 352, loss = 0.08922930\n",
            "Iteration 353, loss = 0.08888154\n",
            "Iteration 354, loss = 0.08856581\n",
            "Iteration 355, loss = 0.08822429\n",
            "Iteration 356, loss = 0.08786492\n",
            "Iteration 357, loss = 0.08753452\n",
            "Iteration 358, loss = 0.08723041\n",
            "Iteration 359, loss = 0.08686721\n",
            "Iteration 360, loss = 0.08654292\n",
            "Iteration 361, loss = 0.08626429\n",
            "Iteration 362, loss = 0.08591198\n",
            "Iteration 363, loss = 0.08560092\n",
            "Iteration 364, loss = 0.08526562\n",
            "Iteration 365, loss = 0.08492886\n",
            "Iteration 366, loss = 0.08460782\n",
            "Iteration 367, loss = 0.08428108\n",
            "Iteration 368, loss = 0.08397786\n",
            "Iteration 369, loss = 0.08366445\n",
            "Iteration 370, loss = 0.08333776\n",
            "Iteration 371, loss = 0.08306250\n",
            "Iteration 372, loss = 0.08275416\n",
            "Iteration 373, loss = 0.08241260\n",
            "Iteration 374, loss = 0.08216335\n",
            "Iteration 375, loss = 0.08179097\n",
            "Iteration 376, loss = 0.08149844\n",
            "Iteration 377, loss = 0.08123949\n",
            "Iteration 378, loss = 0.08087707\n",
            "Iteration 379, loss = 0.08062042\n",
            "Iteration 380, loss = 0.08035277\n",
            "Iteration 381, loss = 0.07996649\n",
            "Iteration 382, loss = 0.07970655\n",
            "Iteration 383, loss = 0.07940195\n",
            "Iteration 384, loss = 0.07911443\n",
            "Iteration 385, loss = 0.07889067\n",
            "Iteration 386, loss = 0.07851188\n",
            "Iteration 387, loss = 0.07819551\n",
            "Iteration 388, loss = 0.07789674\n",
            "Iteration 389, loss = 0.07762111\n",
            "Iteration 390, loss = 0.07736013\n",
            "Iteration 391, loss = 0.07707216\n",
            "Iteration 392, loss = 0.07678749\n",
            "Iteration 393, loss = 0.07647705\n",
            "Iteration 394, loss = 0.07623399\n",
            "Iteration 395, loss = 0.07597910\n",
            "Iteration 396, loss = 0.07567180\n",
            "Iteration 397, loss = 0.07535169\n",
            "Iteration 398, loss = 0.07507669\n",
            "Iteration 399, loss = 0.07481081\n",
            "Iteration 400, loss = 0.07456745\n",
            "Iteration 401, loss = 0.07427103\n",
            "Iteration 402, loss = 0.07398620\n",
            "Iteration 403, loss = 0.07370898\n",
            "Iteration 404, loss = 0.07343302\n",
            "Iteration 405, loss = 0.07316800\n",
            "Iteration 406, loss = 0.07292504\n",
            "Iteration 407, loss = 0.07262501\n",
            "Iteration 408, loss = 0.07240800\n",
            "Iteration 409, loss = 0.07216058\n",
            "Iteration 410, loss = 0.07190537\n",
            "Iteration 411, loss = 0.07159811\n",
            "Iteration 412, loss = 0.07136504\n",
            "Iteration 413, loss = 0.07107980\n",
            "Iteration 414, loss = 0.07082611\n",
            "Iteration 415, loss = 0.07058609\n",
            "Iteration 416, loss = 0.07041220\n",
            "Iteration 417, loss = 0.07007209\n",
            "Iteration 418, loss = 0.06979490\n",
            "Iteration 419, loss = 0.06955918\n",
            "Iteration 420, loss = 0.06930186\n",
            "Iteration 421, loss = 0.06905640\n",
            "Iteration 422, loss = 0.06879975\n",
            "Iteration 423, loss = 0.06855728\n",
            "Iteration 424, loss = 0.06832443\n",
            "Iteration 425, loss = 0.06809693\n",
            "Iteration 426, loss = 0.06783558\n",
            "Iteration 427, loss = 0.06759989\n",
            "Iteration 428, loss = 0.06736160\n",
            "Iteration 429, loss = 0.06711893\n",
            "Iteration 430, loss = 0.06687235\n",
            "Iteration 431, loss = 0.06664751\n",
            "Iteration 432, loss = 0.06642185\n",
            "Iteration 433, loss = 0.06619190\n",
            "Iteration 434, loss = 0.06593194\n",
            "Iteration 435, loss = 0.06570246\n",
            "Iteration 436, loss = 0.06547100\n",
            "Iteration 437, loss = 0.06526100\n",
            "Iteration 438, loss = 0.06506910\n",
            "Iteration 439, loss = 0.06478394\n",
            "Iteration 440, loss = 0.06457440\n",
            "Iteration 441, loss = 0.06434392\n",
            "Iteration 442, loss = 0.06411500\n",
            "Iteration 443, loss = 0.06392070\n",
            "Iteration 444, loss = 0.06367900\n",
            "Iteration 445, loss = 0.06343960\n",
            "Iteration 446, loss = 0.06324678\n",
            "Iteration 447, loss = 0.06302409\n",
            "Iteration 448, loss = 0.06280404\n",
            "Iteration 449, loss = 0.06257080\n",
            "Iteration 450, loss = 0.06245898\n",
            "Iteration 451, loss = 0.06216590\n",
            "Iteration 452, loss = 0.06202215\n",
            "Iteration 453, loss = 0.06175608\n",
            "Iteration 454, loss = 0.06155669\n",
            "Iteration 455, loss = 0.06128294\n",
            "Iteration 456, loss = 0.06109667\n",
            "Iteration 457, loss = 0.06089038\n",
            "Iteration 458, loss = 0.06074450\n",
            "Iteration 459, loss = 0.06051217\n",
            "Iteration 460, loss = 0.06025144\n",
            "Iteration 461, loss = 0.06010604\n",
            "Iteration 462, loss = 0.05987243\n",
            "Iteration 463, loss = 0.05967047\n",
            "Iteration 464, loss = 0.05949771\n",
            "Iteration 465, loss = 0.05927778\n",
            "Iteration 466, loss = 0.05905306\n",
            "Iteration 467, loss = 0.05886949\n",
            "Iteration 468, loss = 0.05864845\n",
            "Iteration 469, loss = 0.05847151\n",
            "Iteration 470, loss = 0.05825559\n",
            "Iteration 471, loss = 0.05809823\n",
            "Iteration 472, loss = 0.05787676\n",
            "Iteration 473, loss = 0.05769128\n",
            "Iteration 474, loss = 0.05750076\n",
            "Iteration 475, loss = 0.05737770\n",
            "Iteration 476, loss = 0.05714438\n",
            "Iteration 477, loss = 0.05697307\n",
            "Iteration 478, loss = 0.05674299\n",
            "Iteration 479, loss = 0.05656337\n",
            "Iteration 480, loss = 0.05640095\n",
            "Iteration 481, loss = 0.05622244\n",
            "Iteration 482, loss = 0.05598956\n",
            "Iteration 483, loss = 0.05581497\n",
            "Iteration 484, loss = 0.05563112\n",
            "Iteration 485, loss = 0.05544511\n",
            "Iteration 486, loss = 0.05530010\n",
            "Iteration 487, loss = 0.05508896\n",
            "Iteration 488, loss = 0.05493804\n",
            "Iteration 489, loss = 0.05478545\n",
            "Iteration 490, loss = 0.05453732\n",
            "Iteration 491, loss = 0.05438229\n",
            "Iteration 492, loss = 0.05418292\n",
            "Iteration 493, loss = 0.05399881\n",
            "Iteration 494, loss = 0.05383197\n",
            "Iteration 495, loss = 0.05366728\n",
            "Iteration 496, loss = 0.05347045\n",
            "Iteration 497, loss = 0.05334841\n",
            "Iteration 498, loss = 0.05311171\n",
            "Iteration 499, loss = 0.05294826\n",
            "Iteration 500, loss = 0.05279347\n",
            "Iteration 501, loss = 0.05264220\n",
            "Iteration 502, loss = 0.05245329\n",
            "Iteration 503, loss = 0.05227957\n",
            "Iteration 504, loss = 0.05210461\n",
            "Iteration 505, loss = 0.05191439\n",
            "Iteration 506, loss = 0.05179028\n",
            "Iteration 507, loss = 0.05159532\n",
            "Iteration 508, loss = 0.05141997\n",
            "Iteration 509, loss = 0.05125285\n",
            "Iteration 510, loss = 0.05109464\n",
            "Iteration 511, loss = 0.05090085\n",
            "Iteration 512, loss = 0.05077050\n",
            "Iteration 513, loss = 0.05060645\n",
            "Iteration 514, loss = 0.05045104\n",
            "Iteration 515, loss = 0.05026846\n",
            "Iteration 516, loss = 0.05012979\n",
            "Iteration 517, loss = 0.05000202\n",
            "Iteration 518, loss = 0.04984426\n",
            "Iteration 519, loss = 0.04963719\n",
            "Iteration 520, loss = 0.04948730\n",
            "Iteration 521, loss = 0.04933657\n",
            "Iteration 522, loss = 0.04916112\n",
            "Iteration 523, loss = 0.04901245\n",
            "Iteration 524, loss = 0.04888561\n",
            "Iteration 525, loss = 0.04872301\n",
            "Iteration 526, loss = 0.04854886\n",
            "Iteration 527, loss = 0.04844944\n",
            "Iteration 528, loss = 0.04820741\n",
            "Iteration 529, loss = 0.04819483\n",
            "Iteration 530, loss = 0.04792750\n",
            "Iteration 531, loss = 0.04778402\n",
            "Iteration 532, loss = 0.04764753\n",
            "Iteration 533, loss = 0.04747294\n",
            "Iteration 534, loss = 0.04731276\n",
            "Iteration 535, loss = 0.04717441\n",
            "Iteration 536, loss = 0.04706862\n",
            "Iteration 537, loss = 0.04686694\n",
            "Iteration 538, loss = 0.04675265\n",
            "Iteration 539, loss = 0.04656836\n",
            "Iteration 540, loss = 0.04644855\n",
            "Iteration 541, loss = 0.04636659\n",
            "Iteration 542, loss = 0.04618626\n",
            "Iteration 543, loss = 0.04603168\n",
            "Iteration 544, loss = 0.04590488\n",
            "Iteration 545, loss = 0.04571935\n",
            "Iteration 546, loss = 0.04561651\n",
            "Iteration 547, loss = 0.04548043\n",
            "Iteration 548, loss = 0.04528475\n",
            "Iteration 549, loss = 0.04518129\n",
            "Iteration 550, loss = 0.04501824\n",
            "Iteration 551, loss = 0.04485550\n",
            "Iteration 552, loss = 0.04471316\n",
            "Iteration 553, loss = 0.04468527\n",
            "Iteration 554, loss = 0.04449345\n",
            "Iteration 555, loss = 0.04430895\n",
            "Iteration 556, loss = 0.04417302\n",
            "Iteration 557, loss = 0.04404421\n",
            "Iteration 558, loss = 0.04395935\n",
            "Iteration 559, loss = 0.04382972\n",
            "Iteration 560, loss = 0.04370096\n",
            "Iteration 561, loss = 0.04351390\n",
            "Iteration 562, loss = 0.04339530\n",
            "Iteration 563, loss = 0.04326484\n",
            "Iteration 564, loss = 0.04317556\n",
            "Iteration 565, loss = 0.04300468\n",
            "Iteration 566, loss = 0.04288684\n",
            "Iteration 567, loss = 0.04276884\n",
            "Iteration 568, loss = 0.04258953\n",
            "Iteration 569, loss = 0.04247548\n",
            "Iteration 570, loss = 0.04236024\n",
            "Iteration 571, loss = 0.04225459\n",
            "Iteration 572, loss = 0.04208034\n",
            "Iteration 573, loss = 0.04199595\n",
            "Iteration 574, loss = 0.04185320\n",
            "Iteration 575, loss = 0.04172284\n",
            "Iteration 576, loss = 0.04165801\n",
            "Iteration 577, loss = 0.04146238\n",
            "Iteration 578, loss = 0.04140910\n",
            "Iteration 579, loss = 0.04123861\n",
            "Iteration 580, loss = 0.04111389\n",
            "Iteration 581, loss = 0.04096051\n",
            "Iteration 582, loss = 0.04080620\n",
            "Iteration 583, loss = 0.04071414\n",
            "Iteration 584, loss = 0.04063970\n",
            "Iteration 585, loss = 0.04048818\n",
            "Iteration 586, loss = 0.04036952\n",
            "Iteration 587, loss = 0.04026326\n",
            "Iteration 588, loss = 0.04012216\n",
            "Iteration 589, loss = 0.04001412\n",
            "Iteration 590, loss = 0.03987076\n",
            "Iteration 591, loss = 0.03977614\n",
            "Iteration 592, loss = 0.03964057\n",
            "Iteration 593, loss = 0.03952006\n",
            "Iteration 594, loss = 0.03943570\n",
            "Iteration 595, loss = 0.03936657\n",
            "Iteration 596, loss = 0.03919979\n",
            "Iteration 597, loss = 0.03907980\n",
            "Iteration 598, loss = 0.03895605\n",
            "Iteration 599, loss = 0.03882272\n",
            "Iteration 600, loss = 0.03869184\n",
            "Iteration 601, loss = 0.03860191\n",
            "Iteration 602, loss = 0.03849905\n",
            "Iteration 603, loss = 0.03839273\n",
            "Iteration 604, loss = 0.03827385\n",
            "Iteration 605, loss = 0.03817689\n",
            "Iteration 606, loss = 0.03806588\n",
            "Iteration 607, loss = 0.03794908\n",
            "Iteration 608, loss = 0.03786887\n",
            "Iteration 609, loss = 0.03775086\n",
            "Iteration 610, loss = 0.03760924\n",
            "Iteration 611, loss = 0.03751843\n",
            "Iteration 612, loss = 0.03738404\n",
            "Iteration 613, loss = 0.03732892\n",
            "Iteration 614, loss = 0.03716410\n",
            "Iteration 615, loss = 0.03702686\n",
            "Iteration 616, loss = 0.03698821\n",
            "Iteration 617, loss = 0.03686164\n",
            "Iteration 618, loss = 0.03676261\n",
            "Iteration 619, loss = 0.03663690\n",
            "Iteration 620, loss = 0.03661639\n",
            "Iteration 621, loss = 0.03643198\n",
            "Iteration 622, loss = 0.03629693\n",
            "Iteration 623, loss = 0.03622125\n",
            "Iteration 624, loss = 0.03613740\n",
            "Iteration 625, loss = 0.03600435\n",
            "Iteration 626, loss = 0.03592227\n",
            "Iteration 627, loss = 0.03588230\n",
            "Iteration 628, loss = 0.03571382\n",
            "Iteration 629, loss = 0.03561423\n",
            "Iteration 630, loss = 0.03550908\n",
            "Iteration 631, loss = 0.03540649\n",
            "Iteration 632, loss = 0.03528508\n",
            "Iteration 633, loss = 0.03522476\n",
            "Iteration 634, loss = 0.03513725\n",
            "Iteration 635, loss = 0.03501982\n",
            "Iteration 636, loss = 0.03493623\n",
            "Iteration 637, loss = 0.03478455\n",
            "Iteration 638, loss = 0.03468526\n",
            "Iteration 639, loss = 0.03458889\n",
            "Iteration 640, loss = 0.03451861\n",
            "Iteration 641, loss = 0.03441333\n",
            "Iteration 642, loss = 0.03432249\n",
            "Iteration 643, loss = 0.03423923\n",
            "Iteration 644, loss = 0.03414889\n",
            "Iteration 645, loss = 0.03405562\n",
            "Iteration 646, loss = 0.03389307\n",
            "Iteration 647, loss = 0.03384009\n",
            "Iteration 648, loss = 0.03376570\n",
            "Iteration 649, loss = 0.03366907\n",
            "Iteration 650, loss = 0.03356904\n",
            "Iteration 651, loss = 0.03344218\n",
            "Iteration 652, loss = 0.03337311\n",
            "Iteration 653, loss = 0.03329642\n",
            "Iteration 654, loss = 0.03315443\n",
            "Iteration 655, loss = 0.03309704\n",
            "Iteration 656, loss = 0.03300168\n",
            "Iteration 657, loss = 0.03291530\n",
            "Iteration 658, loss = 0.03280356\n",
            "Iteration 659, loss = 0.03269204\n",
            "Iteration 660, loss = 0.03263928\n",
            "Iteration 661, loss = 0.03251933\n",
            "Iteration 662, loss = 0.03242259\n",
            "Iteration 663, loss = 0.03238120\n",
            "Iteration 664, loss = 0.03228753\n",
            "Iteration 665, loss = 0.03218613\n",
            "Iteration 666, loss = 0.03211606\n",
            "Iteration 667, loss = 0.03200567\n",
            "Iteration 668, loss = 0.03192260\n",
            "Iteration 669, loss = 0.03186092\n",
            "Iteration 670, loss = 0.03173672\n",
            "Iteration 671, loss = 0.03166815\n",
            "Iteration 672, loss = 0.03157415\n",
            "Iteration 673, loss = 0.03148819\n",
            "Iteration 674, loss = 0.03143874\n",
            "Iteration 675, loss = 0.03132227\n",
            "Iteration 676, loss = 0.03125386\n",
            "Iteration 677, loss = 0.03115991\n",
            "Iteration 678, loss = 0.03111103\n",
            "Iteration 679, loss = 0.03098305\n",
            "Iteration 680, loss = 0.03091198\n",
            "Iteration 681, loss = 0.03084258\n",
            "Iteration 682, loss = 0.03078225\n",
            "Iteration 683, loss = 0.03069180\n",
            "Iteration 684, loss = 0.03058492\n",
            "Iteration 685, loss = 0.03052944\n",
            "Iteration 686, loss = 0.03042898\n",
            "Iteration 687, loss = 0.03036313\n",
            "Iteration 688, loss = 0.03024019\n",
            "Iteration 689, loss = 0.03017726\n",
            "Iteration 690, loss = 0.03007681\n",
            "Iteration 691, loss = 0.03004990\n",
            "Iteration 692, loss = 0.03003761\n",
            "Iteration 693, loss = 0.02985579\n",
            "Iteration 694, loss = 0.02975523\n",
            "Iteration 695, loss = 0.02975022\n",
            "Iteration 696, loss = 0.02961817\n",
            "Iteration 697, loss = 0.02952447\n",
            "Iteration 698, loss = 0.02946300\n",
            "Iteration 699, loss = 0.02936723\n",
            "Iteration 700, loss = 0.02929132\n",
            "Iteration 701, loss = 0.02921972\n",
            "Iteration 702, loss = 0.02915483\n",
            "Iteration 703, loss = 0.02909722\n",
            "Iteration 704, loss = 0.02899256\n",
            "Iteration 705, loss = 0.02890333\n",
            "Iteration 706, loss = 0.02886992\n",
            "Iteration 707, loss = 0.02878295\n",
            "Iteration 708, loss = 0.02866774\n",
            "Iteration 709, loss = 0.02859870\n",
            "Iteration 710, loss = 0.02854745\n",
            "Iteration 711, loss = 0.02847456\n",
            "Iteration 712, loss = 0.02839092\n",
            "Iteration 713, loss = 0.02837378\n",
            "Iteration 714, loss = 0.02823147\n",
            "Iteration 715, loss = 0.02821411\n",
            "Iteration 716, loss = 0.02810926\n",
            "Iteration 717, loss = 0.02803267\n",
            "Iteration 718, loss = 0.02793665\n",
            "Iteration 719, loss = 0.02789370\n",
            "Iteration 720, loss = 0.02780465\n",
            "Iteration 721, loss = 0.02772979\n",
            "Iteration 722, loss = 0.02764578\n",
            "Iteration 723, loss = 0.02759316\n",
            "Iteration 724, loss = 0.02749613\n",
            "Iteration 725, loss = 0.02742365\n",
            "Iteration 726, loss = 0.02737567\n",
            "Iteration 727, loss = 0.02730486\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGCMGVxTOVvb",
        "outputId": "f8315f33-e84b-444f-b4b4-480903c2bcad"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_teste, previsoes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emf6PDjtPJ3x",
        "outputId": "279777c2-c22b-43b8-ee39-e12b52dba7a8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.998"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ConfusionMatrix(rn_credit)\n",
        "cm.fit(x_treino, y_treino)\n",
        "cm.score(x_teste, y_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "QZzVkzkKPTGQ",
        "outputId": "f103709f-231b-4e64-fe4a-b6306378050f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.998"
            ]
          },
          "metadata": {},
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_teste, previsoes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EYmRqfOQULy",
        "outputId": "42bcc825-fb17-491e-ec4b-09fe85516b65"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       436\n",
            "           1       1.00      0.98      0.99        64\n",
            "\n",
            "    accuracy                           1.00       500\n",
            "   macro avg       1.00      0.99      1.00       500\n",
            "weighted avg       1.00      1.00      1.00       500\n",
            "\n"
          ]
        }
      ]
    }
  ]
}